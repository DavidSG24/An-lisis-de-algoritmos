{"title":"**Instrucciones:**","markdown":{"headingText":"**Instrucciones:**","containsRefs":false,"markdown":"\n<h1><b>Reporte Escrito: Experimentos y análisis de estructuras de datos.</b></h1>\n<h1><b>Alumno: David Segundo Garcia</b></h1>\n\n----\n\n\n\nEl análisis de la eficiencia de algoritmos en el álgebra lineal computacional es fundamental para evaluar el rendimiento de diferentes métodos en la resolución de problemas matriciales. La multiplicación de matrices y la eliminación Gaussiana son técnicas ampliamente utilizadas en la computación científica y la inteligencia artificial, por lo que medir su desempeño es clave para seleccionar el enfoque más adecuado según el problema específico.\n\nLa eficiencia de estos algoritmos puede expresarse en términos de complejidad asintótica, donde la multiplicación de matrices estándar tiene una complejidad de $O(n^3)$, mientras que la eliminación Gaussiana se sitúa en $O(n^3)$ en su implementación tradicional. Sin embargo, las pruebas empíricas permiten observar cómo factores como la estructura de la matriz y la implementación afectan el tiempo de ejecución y el número de operaciones realizadas.\n\nEn este trabajo, realizamos un análisis comparativo entre la multiplicación de matrices y la eliminación Gaussiana, midiendo el número de operaciones (multiplicaciones y sumas) y el tiempo de ejecución en matrices aleatorias de diferentes tamaños. Esta evaluación permitirá identificar cuál de los métodos es más eficiente en distintos contextos computacionales y bajo qué condiciones se pueden optimizar.\n\n----\n\n\n**1.** Implementa los siguientes algoritmos sobre matrices.\n\n  * Multiplicación de matrices\n  * Eliminación gaussiana / Gauss-Jordan\n\n\n\n**2.** Compara los desempeños de ambos algoritmos contando el número de operaciones y el tiempo real para matrices aleatorias de tamaño $n×n$\n para $n=100$, $300$, $1000$\n.\n\n**3**. Maneja de manera separada los datos de conteo de operaciones (multiplicaciones y sumas escalares) y las de tiempo real.\n\n**4.** Discute los resultados experimentales:\n\n* ¿Qué puedes concluir?\n* ¿Cuál es el impacto de acceder los elementos contiguos en memoria de una matriz?\n* ¿Qué cambiarías si utilizas matrices dispersas? ¿Cuáles serían los costos?\n\n----\n\n# **Solución**\n\n## **Definimos las funciones que usaremos:**\n\n## **Definiciòn de dimensión de las matrices a analizar y llamar función de análisis**\n\n## **Mostrar resultados**\n\n## **Comparación de complejidades en operaciones matemáticas**\n\n## **Comparación de complejidades en tiempo de ejecución**\n\n# **Análisis de Resultados**\nEl análisis de la eficiencia computacional de los algoritmos implementados para la multiplicación de matrices, la eliminación Gaussiana y la eliminación Gauss-Jordan revela una tendencia clara en términos de complejidad y desempeño.\n\n**Multiplicación de matrices:**\n\n* La multiplicación estándar de matrices sigue un orden de complejidad de\n$𝑂(𝑛^3)$, lo que implica que el número de operaciones crece cúbicamente con respecto al tamaño de la matriz. Esto se evidencia en los datos obtenidos, donde al aumentar el tamaño de la matriz de $𝑛 = 100$ a $𝑛 = 1000$, el número de operaciones se incrementa en un factor de aproximadamente 10^3, confirmando el crecimiento cúbico teórico.\n\n* El tiempo de ejecución aumenta de manera proporcional a la cantidad de operaciones, lo que está alineado con los resultados esperados según la literatura en álgebra computacional (Golub & Van Loan, 2013).\n\n**Eliminación Gaussiana:**\n\n* La eliminación Gaussiana, utilizada para triangularizar matrices, también presenta una complejidad de $𝑂(𝑛^3)$. Sin embargo, en la práctica, su número de operaciones es menor que el de la multiplicación de matrices debido a la naturaleza del algoritmo, que realiza menos iteraciones y multiplicaciones innecesarias.\n* En los resultados experimentales, se observa que el número de operaciones es aproximadamente una fracción del número de operaciones requeridas para la multiplicación de matrices, lo cual concuerda con los métodos de eliminación en sistemas lineales (Trefethen & Bau, 1997).\n\n**Eliminación Gauss-Jordan:**\n\n* La eliminación Gauss-Jordan extiende el método de eliminación Gaussiana hasta obtener la forma reducida de la matriz. Debido a este paso adicional, su número de operaciones es ligeramente superior al de la eliminación Gaussiana estándar, aunque sigue siendo $𝑂(𝑛^3)$.\n* Como se refleja en los resultados, el número de operaciones y el tiempo de ejecución son mayores en comparación con la eliminación Gaussiana, pero menores que los de la multiplicación de matrices. Esto se debe a que, aunque el algoritmo requiere normalizar todas las filas de la matriz, sigue siendo más eficiente que un algoritmo de multiplicación estándar en términos de operaciones aritméticas elementales.\n\n**Comparación general:**\n\nLa gráfica generada confirma la tendencia teórica de crecimiento cúbico en los tres algoritmos. Se observa que la curva de la multiplicación de matrices crece más rápido que la de los otros métodos, mientras que la eliminación Gaussiana y Gauss-Jordan presentan un crecimiento similar pero con un costo computacional adicional en la versión Gauss-Jordan.\nEn términos de aplicabilidad, para la solución de sistemas lineales, la eliminación Gaussiana es más eficiente que la Gauss-Jordan.\nEstos resultados confirman la validez de la teoría de la complejidad computacional aplicada a algoritmos de álgebra lineal y destacan la importancia de seleccionar el método adecuado según el problema a resolver.\n\n## **Discusión de los Resultados Experimentales**\n\n\n**¿Qué puedes concluir?**\n\n  1. Crecimiento cúbico de la complejidad  \n  A partir de los resultados obtenidos, se confirma empíricamente que los tres algoritmos analizados (multiplicación de matrices, eliminación Gaussiana y eliminación Gauss-Jordan) presentan un crecimiento de complejidad del orden $O(n^3)$. Esto es coherente con el análisis teórico, donde la multiplicación de matrices involucra $n^3$ operaciones en su versión estándar, y los métodos de eliminación requieren una cantidad similar de operaciones para la transformación escalonada de la matriz y su posterior normalización (Trefethen & Bau, 1997).\n\n  2. Diferencias en el número de operaciones y tiempo de ejecución  \n  A pesar de compartir la misma complejidad asintótica, la multiplicación de matrices requiere un mayor número de operaciones en comparación con los métodos de eliminación. Esto se debe a la estructura de los algoritmos: en la multiplicación, cada elemento de la matriz resultado requiere $n$ productos escalares, mientras que en los métodos de eliminación se realizan operaciones de normalización y eliminación, reduciendo el número total de multiplicaciones y sumas necesarias (Golub & Van Loan, 2013).\n\n---\n\n**¿Cuál es el impacto de acceder los elementos contiguos en memoria de una matriz?**\n\n1. Localidad espacial y rendimiento en caché  \nEn los algoritmos de álgebra lineal, el acceso eficiente a los datos en memoria es crucial. La **localidad espacial de referencia** se refiere a la tendencia de los programas a acceder a ubicaciones de memoria cercanas entre sí en cortos periodos de tiempo. Cuando una matriz se almacena en memoria en un formato contiguo (fila por fila o columna por columna), el acceso a elementos cercanos minimiza las fallas de caché, lo que mejora significativamente el rendimiento. En términos matemáticos, si consideramos una matriz $A$ de tamaño $n \\times n$ almacenada por filas, los accesos secuenciales a los elementos $A[i, j]$ son más eficientes que acceder a $A[j, i]$ debido a la organización de la memoria (Sedgewick & Wayne, 2011).\n\n2. Impacto en algoritmos de eliminación y multiplicación  \nEn la multiplicación de matrices, si los accesos a la memoria no están optimizados para aprovechar la localidad de referencia, el rendimiento puede disminuir drásticamente debido a un incremento en fallos de caché. En particular, si una matriz se recorre por columnas en lugar de por filas (en un sistema con almacenamiento por filas), se pueden generar accesos no contiguos en memoria, aumentando la latencia de acceso. Esto puede hacer que un algoritmo teóricamente $O(n^3)$ tenga un impacto mayor en la práctica debido a la ineficiencia en la jerarquía de memoria (Patterson & Hennessy, 2017).\n\n---\n\n**¿Qué cambiarías si utilizas matrices dispersas? ¿Cuáles serían los costos?**\n1. Reducción en complejidad y almacenamiento  \nSi en lugar de matrices densas se utilizan **matrices dispersas**, el almacenamiento y la cantidad de operaciones pueden reducirse drásticamente. En una matriz dispersa con solo $k$ elementos no nulos, la multiplicación de matrices puede reducirse a $O(k n)$ en lugar de $O(n^3)$, siempre que se utilicen estructuras eficientes como listas enlazadas o representaciones en formato CSR (Compressed Sparse Row) (Saad, 2003). Esto es particularmente útil en problemas de simulación y procesamiento de grandes volúmenes de datos, donde la mayoría de los elementos son ceros.\n\n2. Costo de implementación y acceso indirecto  \nA pesar de la reducción en la cantidad de operaciones, los algoritmos para manejar matrices dispersas requieren acceso indirecto a los datos mediante índices adicionales. Esto puede generar una sobrecarga en comparación con el acceso secuencial en matrices densas. Además, el almacenamiento en formatos como CSR o CSC (Compressed Sparse Column) introduce una mayor latencia en la recuperación de valores individuales, lo que puede hacer que algunos cálculos sean menos eficientes en términos de tiempo de acceso a memoria (Davis, 2006).\n\n---\n\n# **Referencias Bibliográficas**\n\n- Davis, T. A. (2006). *Direct Methods for Sparse Linear Systems*. SIAM.\n- Golub, G. H., & Van Loan, C. F. (2013). *Matrix Computations* (4th ed.). Johns Hopkins University Press.\n- Patterson, D. A., & Hennessy, J. L. (2017). *Computer Organization and Design: The Hardware/Software Interface* (5th ed.). Morgan Kaufmann.\n- Saad, Y. (2003). *Iterative Methods for Sparse Linear Systems* (2nd ed.). SIAM.\n- Sedgewick, R., & Wayne, K. (2011). *Algorithms* (4th ed.). Addison-Wesley.\n- Trefethen, L. N., & Bau, D. (1997). *Numerical Linear Algebra*. SIAM.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["styles.css"],"output-file":"SegundoDavid_U2_T1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","theme":{"light":"flatly","dark":"darkly"},"toc-location":"right","toc-title":"En esta página"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}