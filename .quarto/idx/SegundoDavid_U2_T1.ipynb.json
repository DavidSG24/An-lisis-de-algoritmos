{"title":"**Instrucciones:**","markdown":{"headingText":"**Instrucciones:**","containsRefs":false,"markdown":"\n<h1><b>Reporte Escrito: Experimentos y an치lisis de estructuras de datos.</b></h1>\n<h1><b>Alumno: David Segundo Garcia</b></h1>\n\n----\n\n\n\nEl an치lisis de la eficiencia de algoritmos en el 치lgebra lineal computacional es fundamental para evaluar el rendimiento de diferentes m칠todos en la resoluci칩n de problemas matriciales. La multiplicaci칩n de matrices y la eliminaci칩n Gaussiana son t칠cnicas ampliamente utilizadas en la computaci칩n cient칤fica y la inteligencia artificial, por lo que medir su desempe침o es clave para seleccionar el enfoque m치s adecuado seg칰n el problema espec칤fico.\n\nLa eficiencia de estos algoritmos puede expresarse en t칠rminos de complejidad asint칩tica, donde la multiplicaci칩n de matrices est치ndar tiene una complejidad de $O(n^3)$, mientras que la eliminaci칩n Gaussiana se sit칰a en $O(n^3)$ en su implementaci칩n tradicional. Sin embargo, las pruebas emp칤ricas permiten observar c칩mo factores como la estructura de la matriz y la implementaci칩n afectan el tiempo de ejecuci칩n y el n칰mero de operaciones realizadas.\n\nEn este trabajo, realizamos un an치lisis comparativo entre la multiplicaci칩n de matrices y la eliminaci칩n Gaussiana, midiendo el n칰mero de operaciones (multiplicaciones y sumas) y el tiempo de ejecuci칩n en matrices aleatorias de diferentes tama침os. Esta evaluaci칩n permitir치 identificar cu치l de los m칠todos es m치s eficiente en distintos contextos computacionales y bajo qu칠 condiciones se pueden optimizar.\n\n----\n\n\n**1.** Implementa los siguientes algoritmos sobre matrices.\n\n  * Multiplicaci칩n de matrices\n  * Eliminaci칩n gaussiana / Gauss-Jordan\n\n\n\n**2.** Compara los desempe침os de ambos algoritmos contando el n칰mero de operaciones y el tiempo real para matrices aleatorias de tama침o $n칑n$\n para $n=100$, $300$, $1000$\n.\n\n**3**. Maneja de manera separada los datos de conteo de operaciones (multiplicaciones y sumas escalares) y las de tiempo real.\n\n**4.** Discute los resultados experimentales:\n\n* 쯈u칠 puedes concluir?\n* 쮺u치l es el impacto de acceder los elementos contiguos en memoria de una matriz?\n* 쯈u칠 cambiar칤as si utilizas matrices dispersas? 쮺u치les ser칤an los costos?\n\n----\n\n# **Soluci칩n**\n\n## **Definimos las funciones que usaremos:**\n\n## **Definici쑕 de dimensi칩n de las matrices a analizar y llamar funci칩n de an치lisis**\n\n## **Mostrar resultados**\n\n## **Comparaci칩n de complejidades en operaciones matem치ticas**\n\n## **Comparaci칩n de complejidades en tiempo de ejecuci칩n**\n\n# **An치lisis de Resultados**\nEl an치lisis de la eficiencia computacional de los algoritmos implementados para la multiplicaci칩n de matrices, la eliminaci칩n Gaussiana y la eliminaci칩n Gauss-Jordan revela una tendencia clara en t칠rminos de complejidad y desempe침o.\n\n**Multiplicaci칩n de matrices:**\n\n* La multiplicaci칩n est치ndar de matrices sigue un orden de complejidad de\n$洧녝(洧녵^3)$, lo que implica que el n칰mero de operaciones crece c칰bicamente con respecto al tama침o de la matriz. Esto se evidencia en los datos obtenidos, donde al aumentar el tama침o de la matriz de $洧녵 = 100$ a $洧녵 = 1000$, el n칰mero de operaciones se incrementa en un factor de aproximadamente 10^3, confirmando el crecimiento c칰bico te칩rico.\n\n* El tiempo de ejecuci칩n aumenta de manera proporcional a la cantidad de operaciones, lo que est치 alineado con los resultados esperados seg칰n la literatura en 치lgebra computacional (Golub & Van Loan, 2013).\n\n**Eliminaci칩n Gaussiana:**\n\n* La eliminaci칩n Gaussiana, utilizada para triangularizar matrices, tambi칠n presenta una complejidad de $洧녝(洧녵^3)$. Sin embargo, en la pr치ctica, su n칰mero de operaciones es menor que el de la multiplicaci칩n de matrices debido a la naturaleza del algoritmo, que realiza menos iteraciones y multiplicaciones innecesarias.\n* En los resultados experimentales, se observa que el n칰mero de operaciones es aproximadamente una fracci칩n del n칰mero de operaciones requeridas para la multiplicaci칩n de matrices, lo cual concuerda con los m칠todos de eliminaci칩n en sistemas lineales (Trefethen & Bau, 1997).\n\n**Eliminaci칩n Gauss-Jordan:**\n\n* La eliminaci칩n Gauss-Jordan extiende el m칠todo de eliminaci칩n Gaussiana hasta obtener la forma reducida de la matriz. Debido a este paso adicional, su n칰mero de operaciones es ligeramente superior al de la eliminaci칩n Gaussiana est치ndar, aunque sigue siendo $洧녝(洧녵^3)$.\n* Como se refleja en los resultados, el n칰mero de operaciones y el tiempo de ejecuci칩n son mayores en comparaci칩n con la eliminaci칩n Gaussiana, pero menores que los de la multiplicaci칩n de matrices. Esto se debe a que, aunque el algoritmo requiere normalizar todas las filas de la matriz, sigue siendo m치s eficiente que un algoritmo de multiplicaci칩n est치ndar en t칠rminos de operaciones aritm칠ticas elementales.\n\n**Comparaci칩n general:**\n\nLa gr치fica generada confirma la tendencia te칩rica de crecimiento c칰bico en los tres algoritmos. Se observa que la curva de la multiplicaci칩n de matrices crece m치s r치pido que la de los otros m칠todos, mientras que la eliminaci칩n Gaussiana y Gauss-Jordan presentan un crecimiento similar pero con un costo computacional adicional en la versi칩n Gauss-Jordan.\nEn t칠rminos de aplicabilidad, para la soluci칩n de sistemas lineales, la eliminaci칩n Gaussiana es m치s eficiente que la Gauss-Jordan.\nEstos resultados confirman la validez de la teor칤a de la complejidad computacional aplicada a algoritmos de 치lgebra lineal y destacan la importancia de seleccionar el m칠todo adecuado seg칰n el problema a resolver.\n\n## **Discusi칩n de los Resultados Experimentales**\n\n\n**쯈u칠 puedes concluir?**\n\n  1. Crecimiento c칰bico de la complejidad  \n  A partir de los resultados obtenidos, se confirma emp칤ricamente que los tres algoritmos analizados (multiplicaci칩n de matrices, eliminaci칩n Gaussiana y eliminaci칩n Gauss-Jordan) presentan un crecimiento de complejidad del orden $O(n^3)$. Esto es coherente con el an치lisis te칩rico, donde la multiplicaci칩n de matrices involucra $n^3$ operaciones en su versi칩n est치ndar, y los m칠todos de eliminaci칩n requieren una cantidad similar de operaciones para la transformaci칩n escalonada de la matriz y su posterior normalizaci칩n (Trefethen & Bau, 1997).\n\n  2. Diferencias en el n칰mero de operaciones y tiempo de ejecuci칩n  \n  A pesar de compartir la misma complejidad asint칩tica, la multiplicaci칩n de matrices requiere un mayor n칰mero de operaciones en comparaci칩n con los m칠todos de eliminaci칩n. Esto se debe a la estructura de los algoritmos: en la multiplicaci칩n, cada elemento de la matriz resultado requiere $n$ productos escalares, mientras que en los m칠todos de eliminaci칩n se realizan operaciones de normalizaci칩n y eliminaci칩n, reduciendo el n칰mero total de multiplicaciones y sumas necesarias (Golub & Van Loan, 2013).\n\n---\n\n**쮺u치l es el impacto de acceder los elementos contiguos en memoria de una matriz?**\n\n1. Localidad espacial y rendimiento en cach칠  \nEn los algoritmos de 치lgebra lineal, el acceso eficiente a los datos en memoria es crucial. La **localidad espacial de referencia** se refiere a la tendencia de los programas a acceder a ubicaciones de memoria cercanas entre s칤 en cortos periodos de tiempo. Cuando una matriz se almacena en memoria en un formato contiguo (fila por fila o columna por columna), el acceso a elementos cercanos minimiza las fallas de cach칠, lo que mejora significativamente el rendimiento. En t칠rminos matem치ticos, si consideramos una matriz $A$ de tama침o $n \\times n$ almacenada por filas, los accesos secuenciales a los elementos $A[i, j]$ son m치s eficientes que acceder a $A[j, i]$ debido a la organizaci칩n de la memoria (Sedgewick & Wayne, 2011).\n\n2. Impacto en algoritmos de eliminaci칩n y multiplicaci칩n  \nEn la multiplicaci칩n de matrices, si los accesos a la memoria no est치n optimizados para aprovechar la localidad de referencia, el rendimiento puede disminuir dr치sticamente debido a un incremento en fallos de cach칠. En particular, si una matriz se recorre por columnas en lugar de por filas (en un sistema con almacenamiento por filas), se pueden generar accesos no contiguos en memoria, aumentando la latencia de acceso. Esto puede hacer que un algoritmo te칩ricamente $O(n^3)$ tenga un impacto mayor en la pr치ctica debido a la ineficiencia en la jerarqu칤a de memoria (Patterson & Hennessy, 2017).\n\n---\n\n**쯈u칠 cambiar칤as si utilizas matrices dispersas? 쮺u치les ser칤an los costos?**\n1. Reducci칩n en complejidad y almacenamiento  \nSi en lugar de matrices densas se utilizan **matrices dispersas**, el almacenamiento y la cantidad de operaciones pueden reducirse dr치sticamente. En una matriz dispersa con solo $k$ elementos no nulos, la multiplicaci칩n de matrices puede reducirse a $O(k n)$ en lugar de $O(n^3)$, siempre que se utilicen estructuras eficientes como listas enlazadas o representaciones en formato CSR (Compressed Sparse Row) (Saad, 2003). Esto es particularmente 칰til en problemas de simulaci칩n y procesamiento de grandes vol칰menes de datos, donde la mayor칤a de los elementos son ceros.\n\n2. Costo de implementaci칩n y acceso indirecto  \nA pesar de la reducci칩n en la cantidad de operaciones, los algoritmos para manejar matrices dispersas requieren acceso indirecto a los datos mediante 칤ndices adicionales. Esto puede generar una sobrecarga en comparaci칩n con el acceso secuencial en matrices densas. Adem치s, el almacenamiento en formatos como CSR o CSC (Compressed Sparse Column) introduce una mayor latencia en la recuperaci칩n de valores individuales, lo que puede hacer que algunos c치lculos sean menos eficientes en t칠rminos de tiempo de acceso a memoria (Davis, 2006).\n\n---\n\n# **Referencias Bibliogr치ficas**\n\n- Davis, T. A. (2006). *Direct Methods for Sparse Linear Systems*. SIAM.\n- Golub, G. H., & Van Loan, C. F. (2013). *Matrix Computations* (4th ed.). Johns Hopkins University Press.\n- Patterson, D. A., & Hennessy, J. L. (2017). *Computer Organization and Design: The Hardware/Software Interface* (5th ed.). Morgan Kaufmann.\n- Saad, Y. (2003). *Iterative Methods for Sparse Linear Systems* (2nd ed.). SIAM.\n- Sedgewick, R., & Wayne, K. (2011). *Algorithms* (4th ed.). Addison-Wesley.\n- Trefethen, L. N., & Bau, D. (1997). *Numerical Linear Algebra*. SIAM.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"css":["styles.css"],"output-file":"SegundoDavid_U2_T1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","theme":{"light":"flatly","dark":"darkly"},"toc-location":"right","toc-title":"En esta p치gina"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}